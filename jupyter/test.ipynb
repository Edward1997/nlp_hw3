{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jhen\n",
      "[nltk_data]     Hao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # Batch size for training.\n",
    "epochs = 1  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding spac\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "embedding_dim = 300\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'data/clean_train.txt'\n",
    "test_path = 'data/clean_validation.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "test_input_texts = []\n",
    "\n",
    "input_characters = set()\n",
    "input_characters.add('<unk>')\n",
    "\n",
    "target_characters = set()\n",
    "target_characters.add('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    test_lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    # word base\n",
    "    target_text = word_tokenize(target_text)\n",
    "    target_text.insert(0, '<start>')\n",
    "    target_text.append('<end>')\n",
    "    input_text = word_tokenize(input_text)\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "            \n",
    "# test data\n",
    "for line in test_lines:\n",
    "    test_input_text = word_tokenize(line)\n",
    "\n",
    "    test_input_texts.append(test_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for t in input_texts:\n",
    "    texts += t\n",
    "freq = nltk.FreqDist(texts)\n",
    "for key, val in freq.items():\n",
    "    if val == 1:\n",
    "        input_characters.remove(key)\n",
    "\n",
    "texts = []\n",
    "for t in target_texts:\n",
    "    texts += t\n",
    "freq = nltk.FreqDist(texts)\n",
    "for key, val in freq.items():\n",
    "    if val == 1:\n",
    "        target_characters.remove(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 5405\n",
      "Number of unique output tokens: 5398\n",
      "Max sequence length for inputs: 215\n",
      "Max sequence length for outputs: 217\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "input_characters.insert(0,'<pad>')\n",
    "target_characters.insert(0,'<pad>')\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype='int')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype='int')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, 1), dtype='int')\n",
    "\n",
    "test_encoder_input_data = np.zeros((len(test_input_texts), max_encoder_seq_length), dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        if char in input_token_index:\n",
    "            encoder_input_data[i, t] = input_token_index[char]\n",
    "        else:\n",
    "            encoder_input_data[i, t] = input_token_index['<unk>']\n",
    "\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        if char in target_token_index:\n",
    "            index = target_token_index[char]\n",
    "        else:\n",
    "            index = target_token_index['<unk>']\n",
    "        decoder_input_data[i, t] = index\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, 0] = index\n",
    "\n",
    "for i, test_input_text in enumerate(test_input_texts):\n",
    "    for t, char in enumerate(test_input_text):\n",
    "        if t >= max_encoder_seq_length:\n",
    "            break\n",
    "        \n",
    "        if char in input_token_index:\n",
    "            test_encoder_input_data[i, t] = input_token_index[char]\n",
    "        else:\n",
    "            test_encoder_input_data[i, t] = input_token_index['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# embedding layer\n",
    "word2vec_file = \"GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_file, binary=True)\n",
    "# for encoder\n",
    "embedding_matrix = np.zeros((len(input_characters) + 1, embedding_dim))\n",
    "for i, word in enumerate(input_characters):\n",
    "    if word in word2vec:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = word2vec.wv[word]\n",
    "# for decoder\n",
    "de_embedding_matrix = np.zeros((len(target_characters) + 1, embedding_dim))\n",
    "for i, word in enumerate(target_characters):\n",
    "    if word in word2vec:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        de_embedding_matrix[i] = word2vec.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jhen Hao\\.conda\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2974: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "embedding_layer = Embedding(len(input_characters) + 1, embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=None,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "embedding_encoder_inputs = embedding_layer(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(embedding_encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "de_embedding_layer = Embedding(len(target_characters) + 1, embedding_dim,\n",
    "                            weights=[de_embedding_matrix],\n",
    "                            input_length=None,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "embedding_decoder_inputs = de_embedding_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(embedding_decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    embedding_decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # print(states_value.shape)\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['<start>']\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_char = target_characters[sampled_token_index]\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '<end>' or\n",
    "           len(decoded_sentence) >= max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            break\n",
    "        else:\n",
    "            # Update the target sequence (of length 1).\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update states\n",
    "            states_value = [h, c]\n",
    "            \n",
    "        decoded_sentence+=sampled_char + ' '\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('result.txt', 'w', encoding='utf-8')\n",
    "for seq_index in test_encoder_input_data:\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = seq_index\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    f.write(decoded_sentence+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input sentence: ['In', 'my', 'opinion', 'the', 'TV', 'channels', 'should', 'show', 'the', 'violent', 'movies', 'and', 'TV', 'shows', 'in', 'night', ',', 'not', 'in', 'the', 'afternoon', '.']\n",
      "\n",
      "Decoded sentence: The <unk> is a <unk> of the <unk> , <unk> , the <unk> , <unk> , the <unk> , <unk> . \n",
      "---\n",
      "Input sentence: ['There', 'I', 'began', 'to', 'study', 'Family', 'Therapy', ',', 'discipline', 'that', 'I', 'enjoy', 'too', 'much', ',', 'because', 'I', \"'ve\", 'seen', 'I', 'can', 'help', 'people', 'about', 'their', 'troubles', '..']\n",
      "\n",
      "Decoded sentence: In the <unk> , I have a lot of the <unk> of the <unk> , and the <unk> , <unk> , the <unk> , <unk> . \n",
      "---\n",
      "Input sentence: ['It', 'seems', 'me', ',', 'that', 'in', 'this', 'song', 'there', 'are', 'romantic', 'notes', '.']\n",
      "\n",
      "Decoded sentence: The <unk> is a <unk> of the <unk> of the <unk> . \n",
      "---\n",
      "Input sentence: ['So', ',', 'I', 'want', 'to', 'say', 'that', 'I', 'admire', 'your', 'right', 'disposition', 'and', 'sacrifice', 'in', 'solve', 'the', 'problems', 'related', 'a', 'your', 'phobia', ',', 'as', 'for', 'example', 'going', 'out', 'the', 'company', '.']\n",
      "\n",
      "Decoded sentence: The <unk> is a <unk> of the <unk> , I have a lot of the <unk> of the <unk> , and the <unk> , <unk> , <unk> , <unk> . \n",
      "---\n",
      "Input sentence: ['I', 'graduated', 'Chung', 'university', '.']\n",
      "\n",
      "Decoded sentence: I 'm to go to music . \n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(5):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('---')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('')\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
